{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzLigizi3868K9X5nasSTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidj4tech/ollama/blob/main/ollama_stackoverflow_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6OxiB1Za0J7",
        "outputId": "814d0f51-1009-4e8f-c0f3-d68563ba1889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> starting ollama serve\n",
            ">>> starting ngrok http --log stderr 11434\n",
            "2024/05/24 00:53:07 routes.go:1008: INFO server config env=\"map[OLLAMA_DEBUG:false OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:*] OLLAMA_RUNNERS_DIR: OLLAMA_TMPDIR:]\"\n",
            "time=2024-05-24T00:53:07.425Z level=INFO source=images.go:704 msg=\"total blobs: 0\"\n",
            "time=2024-05-24T00:53:07.426Z level=INFO source=images.go:711 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-05-24T00:53:07.426Z level=INFO source=routes.go:1054 msg=\"Listening on [::]:11434 (version 0.1.38)\"\n",
            "time=2024-05-24T00:53:07.426Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama837291346/runners\n",
            "t=2024-05-24T00:53:07+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "t=2024-05-24T00:53:07+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "t=2024-05-24T00:53:07+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "t=2024-05-24T00:53:07+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "t=2024-05-24T00:53:07+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "t=2024-05-24T00:53:07+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2024-05-24T00:53:07+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://a18e-35-224-255-38.ngrok-free.app\n",
            "time=2024-05-24T00:53:14.980Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu_avx2 cuda_v11 rocm_v60002 cpu cpu_avx]\"\n",
            "time=2024-05-24T00:53:15.251Z level=INFO source=types.go:71 msg=\"inference compute\" id=GPU-e31ec021-60a3-6126-5f4b-400506fa6a8c library=cuda compute=7.5 driver=12.2 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n",
            "t=2024-05-24T02:35:41+0000 lvl=info msg=\"join connections\" obj=join id=870977c2fd7d l=127.0.0.1:11434 r=58.178.172.155:59710\n",
            "[GIN] 2024/05/24 - 02:35:41 | 200 |      56.552Âµs |  58.178.172.155 | HEAD     \"/\"\n",
            "time=2024-05-24T02:35:43.316Z level=INFO source=download.go:136 msg=\"downloading 4fed7364ee3e in 24 100 MB part(s)\"\n",
            "time=2024-05-24T02:36:03.068Z level=INFO source=download.go:136 msg=\"downloading c608dc615584 in 1 149 B part(s)\"\n",
            "time=2024-05-24T02:36:04.829Z level=INFO source=download.go:136 msg=\"downloading fa8235e5b48f in 1 1.1 KB part(s)\"\n",
            "time=2024-05-24T02:36:06.588Z level=INFO source=download.go:136 msg=\"downloading d47ab88b61ba in 1 140 B part(s)\"\n",
            "time=2024-05-24T02:36:08.291Z level=INFO source=download.go:136 msg=\"downloading f7eda1da5a81 in 1 485 B part(s)\"\n",
            "[GIN] 2024/05/24 - 02:36:17 | 200 | 35.936206681s |  58.178.172.155 | POST     \"/api/pull\"\n"
          ]
        }
      ],
      "source": [
        "# Source: https://stackoverflow.com/questions/77697302/how-to-run-ollama-in-google-colab\n",
        "\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token('2foqQkjl46qZqGzgBWOkrbsQV1s_4Y5owHC3kfRYRHRk1GwcG')\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "os.environ.update({'OLLAMA_HOST': '0.0.0.0'})\n",
        "os.environ.update({'OLLAMA_PORT': '11434'})\n",
        "os.environ.update({'OLLAMA_ORIGINS': '*'})\n",
        "\n",
        "\n",
        "async def run_process(cmd):\n",
        "  print('>>> starting', *cmd)\n",
        "  p = await asyncio.subprocess.create_subprocess_exec(\n",
        "      *cmd,\n",
        "      stdout=asyncio.subprocess.PIPE,\n",
        "      stderr=asyncio.subprocess.PIPE,\n",
        "  )\n",
        "\n",
        "  async def pipe(lines):\n",
        "    async for line in lines:\n",
        "      print(line.strip().decode('utf-8'))\n",
        "\n",
        "  await asyncio.gather(\n",
        "      pipe(p.stdout),\n",
        "      pipe(p.stderr),\n",
        "  )\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "await asyncio.gather(\n",
        "    run_process(['ollama', 'serve']),\n",
        "    run_process(['ngrok', 'http', '--log', 'stderr', '11434']),\n",
        ")"
      ]
    }
  ]
}